{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder \n",
    "\n",
    "Autoencoder (AE) is a method to map high-dimensional signal into lower-dimensional latent space [1]. However, for signal creation it is not strightforward to sample a low-dimensional signal from latent space, and generate a high-dimensional signal that follows the distribution of training data.  \n",
    "\n",
    "## Autoencoder\n",
    "\n",
    "A simple autoencoder architecture consists of three major components:\n",
    "- Encoder \n",
    "- Latent Representation \n",
    "- Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different from AE which maps the input to a vector, variatoinal autoencoder (VAE) restricts the latent representation only as a distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evidence Lower Bound \n",
    "\n",
    "\n",
    "The Evidence Lower Bound (ELBO) is a concept primarily used in the context of variational inference, a method for approximating complex probability distributions. In probabilistic modeling, we often want to infer the posterior distribution of latent variables given observed data. However, calculating the true posterior distribution is often unfeasible. Variational inference approximates the posterior distribution with a simpler distribution chosen from a parameterized function, such as a Gaussian distribution. \n",
    "\n",
    "The ELBO serves as a lower bound for the log marginal likelihood of the data. By maximizing the ELBO with respect to the parameters of the approximate posterior distribution, we indirectly maximize the log marginal likelihood. This is because the ELBO is derived from the Kullback-Leibler (KL) divergence between the approximate posterior and the true posterior, and maximizing the ELBO minimizes this divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varitaional Autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader \n",
    "#from torchvision.utils import save_image, make_grid\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "datapath = r'C:\\Users\\User\\Documents\\repos\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_dim = 784\n",
    "hidden_dim = 200\n",
    "latent_dim = 20\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "lr = 3e-4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    #transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(datapath, train=True, transform=transform, download=False)\n",
    "test_dataset = datasets.MNIST(datapath, train=False, transform=transform, download=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU() \n",
    "        \n",
    "\n",
    "    def encoder(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        mu = self.fc_mean(x)\n",
    "        logvar = self.fc_var(x)\n",
    "        return mu, logvar \n",
    "\n",
    "\n",
    "    def decoder(self, x):\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = torch.sigmoid(self.fc5(x))\n",
    "        return x \n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(device)\n",
    "        z = mean + epsilon * var\n",
    "        return z \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, torch.exp(0.5 * logvar)) \n",
    "        recon = self.decoder(z)\n",
    "        return recon, mu, logvar \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(input_dim, hidden_dim, latent_dim).to(device) \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#loss_fn = nn.BCELoss(reduction=\"sum\")|\n",
    "\n",
    "def loss_function(x, x_hat, mean, log_var):\n",
    "    reproduction_loss = F.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "    KLD      = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
    "    return reproduction_loss + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for idx, (x, _) in enumerate(train_loader):\n",
    "        x = x.view(batch_size, input_dim)\n",
    "        x = x.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x_hat, mean, log_var = model(x)\n",
    "        \n",
    "        loss = loss_function(x, x_hat, mean, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference \n",
    "\n",
    "- https://kvfrans.com/variational-autoencoders-explained/\n",
    "- https://www.jeremyjordan.me/variational-autoencoders/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
